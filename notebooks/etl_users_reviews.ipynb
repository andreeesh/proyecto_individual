{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/andreeesh/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/andreeesh/HenryDataScience/proyecto_individual_steam/notebooks/etl_users_reviews.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andreeesh/HenryDataScience/proyecto_individual_steam/notebooks/etl_users_reviews.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Eliminar duplicados\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andreeesh/HenryDataScience/proyecto_individual_steam/notebooks/etl_users_reviews.ipynb#W0sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m clean_items \u001b[39m=\u001b[39m normalized\u001b[39m.\u001b[39mdrop_duplicates(keep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andreeesh/HenryDataScience/proyecto_individual_steam/notebooks/etl_users_reviews.ipynb#W0sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m clean_items[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m clean_items[\u001b[39m'\u001b[39;49m\u001b[39mreview\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(analyze_sentiment)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andreeesh/HenryDataScience/proyecto_individual_steam/notebooks/etl_users_reviews.ipynb#W0sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m clean_items[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfillna(\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andreeesh/HenryDataScience/proyecto_individual_steam/notebooks/etl_users_reviews.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Dividir el DataFrame en bloques de 1000 filas\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/home/andreeesh/HenryDataScience/proyecto_individual_steam/notebooks/etl_users_reviews.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andreeesh/HenryDataScience/proyecto_individual_steam/notebooks/etl_users_reviews.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39manalyze_sentiment\u001b[39m(text):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/andreeesh/HenryDataScience/proyecto_individual_steam/notebooks/etl_users_reviews.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \tanalyzer \u001b[39m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andreeesh/HenryDataScience/proyecto_individual_steam/notebooks/etl_users_reviews.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \tsentiment \u001b[39m=\u001b[39m analyzer\u001b[39m.\u001b[39mpolarity_scores(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andreeesh/HenryDataScience/proyecto_individual_steam/notebooks/etl_users_reviews.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \t\u001b[39mif\u001b[39;00m sentiment[\u001b[39m'\u001b[39m\u001b[39mcompound\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.05\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/nltk/sentiment/vader.py:341\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     lexicon_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m ):\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlexicon_file \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mload(lexicon_file)\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlexicon \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_lex_dict()\n\u001b[1;32m    342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstants \u001b[39m=\u001b[39m VaderConstants()\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/nltk/sentiment/vader.py:351\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.make_lex_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlexicon_file\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    350\u001b[0m     (word, measure) \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m]\n\u001b[0;32m--> 351\u001b[0m     lex_dict[word] \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(measure)\n\u001b[1;32m    352\u001b[0m \u001b[39mreturn\u001b[39;00m lex_dict\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ETL australian_user_reviews.csv\n",
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Define la función para realizar el análisis de sentimiento\n",
    "def analyze_sentiment(text):\n",
    "\tanalyzer = SentimentIntensityAnalyzer()\n",
    "\tsentiment = analyzer.polarity_scores(text)\n",
    "\tif sentiment['compound'] >= 0.05:\n",
    "\t\treturn 2\n",
    "\telif sentiment['compound'] <= -0.05:\n",
    "\t\treturn 0\n",
    "\telse:\n",
    "\t\treturn 1\n",
    "\n",
    "user_reviews = []\n",
    "\n",
    "# Abre el archivo y recorrerlo para agregar las reseñas a la lista\n",
    "with open('../data_sources/json/australian_user_reviews.json', encoding='utf-8') as f:\n",
    "\tfor line in f:\n",
    "\t\tobject = json.loads(json.dumps(ast.literal_eval(line)))\n",
    "\t\tuser_reviews.append(object)\n",
    "\n",
    "# Crea el dataframe a partir de la lista\n",
    "df_user_reviews = pd.DataFrame(user_reviews)\n",
    "\n",
    "# Normaliza la columna reviews\n",
    "normalized = pd.json_normalize(user_reviews, record_path=['reviews'], meta=['user_id'] )\n",
    "\n",
    "# Elimina las filas vacias\n",
    "normalized = normalized.dropna()\n",
    "\n",
    "# Eliminar duplicados\n",
    "clean_items = normalized.drop_duplicates(keep='first')\n",
    "\n",
    "clean_items['review'] = clean_items['review'].apply(analyze_sentiment)\n",
    "clean_items['review'].fillna(1, inplace=True)\n",
    "\n",
    "# Dividir el DataFrame en bloques de 1000 filas\n",
    "rows_per_file = 10000\n",
    "total_rows = len(clean_items)\n",
    "total_files = math.ceil(total_rows / rows_per_file)\n",
    "\n",
    "# Recorrer y crear archivos Parquet\n",
    "for i in range(total_files):\n",
    "    s = i * rows_per_file\n",
    "    e = min((i + 1) * rows_per_file, total_rows)\n",
    "    block = clean_items.iloc[s:e]\n",
    "\n",
    "    # Crear el nombre del archivo Parquet (puedes modificar el nombre como desees)\n",
    "    file_name = f\"../data_sources/parquet/user_reviews/dataset_{i + 1}.parquet\"\n",
    "\n",
    "    # Guardar el bloque actual en un archivo Parquet\n",
    "    block.to_parquet(file_name)\n",
    "\n",
    "    print(f\"Archivo {file_name} guardado con éxito. Filas {s + 1} a {e} del DataFrame.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
